{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Exploration Kickstarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "votre taille d'écran 131072x1 est fausse. Attendez-vous à des problèmes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/27 21:02:26 WARN Utils: Your hostname, DESKTOP-NIT8A65 resolves to a loopback address: 127.0.1.1; using 172.25.180.10 instead (on interface eth0)\n",
      "22/11/27 21:02:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/27 21:02:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", \"True\").csv(\"/home/mgard/Documents/pyspark/cours-spark-telecom/data/train_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes: 108129\n",
      "Nombre de colonnes: 14\n"
     ]
    }
   ],
   "source": [
    "print(f'Nombre de lignes: {df.count()}')\n",
    "print(f\"Nombre de colonnes: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+------+--------------------+---------------------+-------+--------+----------+----------------+----------+-----------+-------------+------------+\n",
      "|    project_id|                name|                desc|  goal|            keywords|disable_communication|country|currency|  deadline|state_changed_at|created_at|launched_at|backers_count|final_status|\n",
      "+--------------+--------------------+--------------------+------+--------------------+---------------------+-------+--------+----------+----------------+----------+-----------+-------------+------------+\n",
      "|kkst1451568084| drawing for dollars|I like drawing pi...|  20.0| drawing-for-dollars|                False|     US|     USD|1241333999|      1241334017|1240600507| 1240602723|            3|           1|\n",
      "|kkst1474482071|Sponsor Dereck Bl...|I  Dereck Blackbu...| 300.0|sponsor-dereck-bl...|                False|     US|     USD|1242429000|      1242432018|1240960224| 1240975592|            2|           0|\n",
      "| kkst183622197|       Mr. Squiggles|So I saw darkpony...|  30.0|        mr-squiggles|                False|     US|     USD|1243027560|      1243027818|1242163613| 1242164398|            0|           0|\n",
      "| kkst597742710|Help me write my ...|Do your part to h...| 500.0|help-me-write-my-...|                False|     US|     USD|1243555740|      1243556121|1240963795| 1240966730|           18|           1|\n",
      "|kkst1913131122|Support casting m...|I m nearing compl...|2000.0|support-casting-m...|                False|     US|     USD|1243769880|      1243770317|1241177914| 1241180541|            1|           0|\n",
      "|kkst1085176748|        daily digest|I m a fledgling v...| 700.0|        daily-digest|                False|     US|     USD|1243815600|      1243816219|1241050799| 1241464468|           14|           0|\n",
      "|kkst1468954715|iGoozex - Free iP...|I am an independe...| 250.0|igoozex-free-ipho...|                False|     US|     USD|1243872000|      1243872028|1241725172| 1241736308|            2|           0|\n",
      "| kkst194050612|Drive A Faster Ca...|Drive A Faster Ca...|1000.0|drive-a-faster-ca...|                False|     US|     USD|1244088000|      1244088022|1241460541| 1241470291|           32|           1|\n",
      "| kkst708883590|\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"...|Opening Friday  J...|5000.0|lostles-at-tinys-...|                False|     US|     USD|1244264400|      1244264422|1241415164| 1241480901|           44|           0|\n",
      "| kkst890976740|Choose Your Own A...|This project is f...|3500.0|choose-your-own-a...|                False|     US|     USD|1244946540|      1244946632|1242268157| 1242273460|           18|           0|\n",
      "+--------------+--------------------+--------------------+------+--------------------+---------------------+-------+--------+----------+----------------+----------+-----------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: string (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- disable_communication: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- deadline: string (nullable = true)\n",
      " |-- state_changed_at: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- launched_at: string (nullable = true)\n",
      " |-- backers_count: integer (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données\n",
    "\n",
    "L’ensemble des ressources nécessaires pour les prochaines questions se trouvent dans la documentation de Spark.\n",
    "\n",
    "Chargez le fichier train_clean.csv dans un DataFrame. La première ligne du fichier donne le nom de chaque colonne (aka le header), on veut que cette ligne soit utilisée pour nommer les colonnes du dataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcasted = df.withColumn(\"goal\", df.goal.cast(\"int\")) \\\n",
    "             .withColumn(\"deadline\", df.deadline.cast(\"int\")) \\\n",
    "             .withColumn(\"state_changed_at\", df.state_changed_at.cast(\"int\")) \\\n",
    "             .withColumn(\"created_at\", df.created_at.cast(\"int\")) \\\n",
    "             .withColumn(\"launched_at\", df.launched_at.cast(\"int\")) \\\n",
    "             .withColumn(\"backers_count\", df.backers_count.cast(\"int\")) \\\n",
    "             .withColumn(\"final_status\", df.final_status.cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- goal: integer (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- disable_communication: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- deadline: integer (nullable = true)\n",
      " |-- state_changed_at: integer (nullable = true)\n",
      " |-- created_at: integer (nullable = true)\n",
      " |-- launched_at: integer (nullable = true)\n",
      " |-- backers_count: integer (nullable = true)\n",
      " |-- final_status: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfcasted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "Certaines opérations sur les colonnes sont déjà implémentées dans Spark, mais il est souvent nécessaire de faire appel à des fonctions plus complexes. Dans ce cas on peut créer des UDFs (User Defined Functions) qui permettent d’implémenter de nouvelles opérations sur les colonnes. Voir la partie User Defined Functions du fichier spark_notes.md pour comprendre comment ça fonctionne.\n",
    "\n",
    "Affichez une description statistique des colonnes de type Int :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+--------------------+\n",
      "|summary|             goal|      backers_count|        final_status|\n",
      "+-------+-----------------+-------------------+--------------------+\n",
      "|  count|           107615|             108128|              108128|\n",
      "|   mean|36839.03430748502|  6434187.413250962|  1052360.7834973366|\n",
      "| stddev|974215.3015529716|9.324061726649418E7|3.7760499401841596E7|\n",
      "|    min|                0|                  0|                   0|\n",
      "|    max|        100000000|         1430423170|          1428977971|\n",
      "+-------+-----------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfcasted.select(\"goal\", \"backers_count\", \"final_status\") \\\n",
    "        .describe() \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observez les autres colonnes, posez-vous les bonnes questions : quel cleaning faire pour chaque colonne ? Y a-t-il des colonnes inutiles ? Comment traiter les valeurs manquantes ? A-t-on des données dupliquées ? Quelles sont les valeurs de mes colonnes ? Des répartitions intéressantes ? Des \"fuites du futur\" (vous entendrez souvent le terme data leakage) ??? Proposez des cleanings à faire sur les données : des groupBy-count, des show, des dropDuplicates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  deadline|\n",
      "+----------+\n",
      "|1254767400|\n",
      "|1263597060|\n",
      "|1274115540|\n",
      "|1274145060|\n",
      "|1280372820|\n",
      "|1281730020|\n",
      "|1281769200|\n",
      "|1283292000|\n",
      "|1286168724|\n",
      "|1287701689|\n",
      "|1288926510|\n",
      "|1289102340|\n",
      "|1290314164|\n",
      "|1290704714|\n",
      "|1291537500|\n",
      "|1292387613|\n",
      "|1292400000|\n",
      "|1293256800|\n",
      "|1297473364|\n",
      "|1297726200|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|             country|count|\n",
      "+--------------------+-----+\n",
      "|                  US|91545|\n",
      "|                  GB| 8743|\n",
      "|                  CA| 3733|\n",
      "|                  AU| 1877|\n",
      "|                  NL|  702|\n",
      "|               False|  428|\n",
      "|                  NZ|  354|\n",
      "|                  SE|  240|\n",
      "|                  DK|  196|\n",
      "|                  NO|  113|\n",
      "|                  IE|  111|\n",
      "|               999.0|    2|\n",
      "| rofl-metaphor-56-ep|    1|\n",
      "|              5000.0|    1|\n",
      "|up-until-now-reco...|    1|\n",
      "|sketches-in-time-...|    1|\n",
      "|in-the-grass-a-sh...|    1|\n",
      "|dave-afdahl-cliff...|    1|\n",
      "|kuya-ko-my-big-br...|    1|\n",
      "|welcome-to-not-mo...|    1|\n",
      "|jason-blum-wester...|    1|\n",
      "|il-fazzoletto-an-...|    1|\n",
      "|jenny-macdonalds-...|    1|\n",
      "|feature-film-some...|    1|\n",
      "|undo-undo-undone-...|    1|\n",
      "|good-job-thanks-a...|    1|\n",
      "|                True|    1|\n",
      "|fight-life-boxing...|    1|\n",
      "|              2500.0|    1|\n",
      "|smile-its-not-tha...|    1|\n",
      "|              1500.0|    1|\n",
      "|A compendium of h...|    1|\n",
      "|hard-lick-sports-...|    1|\n",
      "|bobby-lynchs-debu...|    1|\n",
      "|dont-hang-up-on-m...|    1|\n",
      "|the-nashville-ses...|    1|\n",
      "|meow-meow-maulana...|    1|\n",
      "|way-up-in-vermont...|    1|\n",
      "|              1000.0|    1|\n",
      "|ice-will-reveal-a...|    1|\n",
      "|sexisgoodtv-web-t...|    1|\n",
      "|1st-solo-cd-girls...|    1|\n",
      "|my-village-my-lob...|    1|\n",
      "|help-me-fund-my-n...|    1|\n",
      "|noah-clean-prison...|    1|\n",
      "|the-voice-of-cour...|    1|\n",
      "|the-next-golden-a...|    1|\n",
      "|nadjah-nicoles-de...|    1|\n",
      "|dont-trust-your-s...|    1|\n",
      "|esp-intuition-and...|    1|\n",
      "|together-alone-th...|    1|\n",
      "|all-is-good-then-...|    1|\n",
      "|the-most-loved-a-...|    1|\n",
      "|             70000.0|    1|\n",
      "|going-home-what-a...|    1|\n",
      "|A blend of Melodi...|    1|\n",
      "|chadd-thomas-unfi...|    1|\n",
      "|line-edge-and-for...|    1|\n",
      "|instead-of-sleepi...|    1|\n",
      "|intimacy-with-tre...|    1|\n",
      "|if-the-world-was-...|    1|\n",
      "|Real life stories...|    1|\n",
      "|joe-neary-from-lo...|    1|\n",
      "|promote-validated...|    1|\n",
      "|the-123-with-jami...|    1|\n",
      "|majestys-ep-kings...|    1|\n",
      "|mile-high-a-hilar...|    1|\n",
      "|no-pride-no-shame...|    1|\n",
      "|hannuka-story-the...|    1|\n",
      "|fire-fox-in-radic...|    1|\n",
      "|jack-chaps-dog-de...|    1|\n",
      "|home-is-where-the...|    1|\n",
      "|             20000.0|    1|\n",
      "|elf-tears-war-mag...|    1|\n",
      "|i-speak-fluent-mo...|    1|\n",
      "|randys-vision-for...|    1|\n",
      "|st-francis-mohamm...|    1|\n",
      "|derek-v-the-journ...|    1|\n",
      "|the-swirly-twirli...|    1|\n",
      "|steve-sabos-comed...|    1|\n",
      "|faith-struggle-vi...|    1|\n",
      "|                null|    1|\n",
      "|sexy-krazy-raw-an...|    1|\n",
      "|luna-the-lone-wol...|    1|\n",
      "|center-ice-brewin...|    1|\n",
      "|ms-groundhog-ms-b...|    1|\n",
      "|landslide-erotic-...|    1|\n",
      "|improve-the-commu...|    1|\n",
      "|the-bowhunter-a-s...|    1|\n",
      "|we-the-network-it...|    1|\n",
      "| the-aric-audio-mini|    1|\n",
      "|seven-zero-eight-...|    1|\n",
      "|umeos-the-21st-ce...|    1|\n",
      "|here-without-art-...|    1|\n",
      "|bring-mary-mcdono...|    1|\n",
      "|                  DE|    1|\n",
      "|the-foxhole-an-ac...|    1|\n",
      "+--------------------+-----+\n",
      "\n",
      "+--------------------+-----+\n",
      "|            currency|count|\n",
      "+--------------------+-----+\n",
      "|                 USD|91545|\n",
      "|                 GBP| 8743|\n",
      "|                 CAD| 3733|\n",
      "|                 AUD| 1877|\n",
      "|                 EUR|  814|\n",
      "|                  US|  406|\n",
      "|                 NZD|  354|\n",
      "|                 SEK|  240|\n",
      "|                 DKK|  196|\n",
      "|                 NOK|  113|\n",
      "|               False|   73|\n",
      "|                  GB|   13|\n",
      "|                  AU|    3|\n",
      "|                  CA|    3|\n",
      "|                  NL|    2|\n",
      "|the-artists-proce...|    1|\n",
      "|the-artists-proce...|    1|\n",
      "|the-soloist-first...|    1|\n",
      "|the-arrangement-a...|    1|\n",
      "|the-artists-proce...|    1|\n",
      "|                null|    1|\n",
      "|final-day-a-drama...|    1|\n",
      "|flutter-and-there...|    1|\n",
      "|              6300.0|    1|\n",
      "|jack-the-radio-lo...|    1|\n",
      "|                  NZ|    1|\n",
      "|                  NO|    1|\n",
      "|             77750.0|    1|\n",
      "|              1750.0|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Resolved attribute(s) state_changed_at#26 missing from project_id#17,name#18,desc#19,goal#136,keywords#21,disable_communication#22,country#23,currency#24,deadline#151,state_changed_at#166,created_at#181,launched_at#196,backers_count#211,final_status#226 in operator !Aggregate [state_changed_at#26], [state_changed_at#26, count(1) AS count#750L]. Attribute(s) with the same name appear in the operation: state_changed_at. Please check if the right attribute(s) are used.;\n!Aggregate [state_changed_at#26], [state_changed_at#26, count(1) AS count#750L]\n+- Project [project_id#17, name#18, desc#19, goal#136, keywords#21, disable_communication#22, country#23, currency#24, deadline#151, state_changed_at#166, created_at#181, launched_at#196, backers_count#211, cast(final_status#30 as int) AS final_status#226]\n   +- Project [project_id#17, name#18, desc#19, goal#136, keywords#21, disable_communication#22, country#23, currency#24, deadline#151, state_changed_at#166, created_at#181, launched_at#196, cast(backers_count#29 as int) AS backers_count#211, final_status#30]\n      +- Project [project_id#17, name#18, desc#19, goal#136, keywords#21, disable_communication#22, country#23, currency#24, deadline#151, state_changed_at#166, created_at#181, cast(launched_at#28 as int) AS launched_at#196, backers_count#29, final_status#30]\n         +- Project [project_id#17, name#18, desc#19, goal#136, keywords#21, disable_communication#22, country#23, currency#24, deadline#151, state_changed_at#166, cast(created_at#27 as int) AS created_at#181, launched_at#28, backers_count#29, final_status#30]\n            +- Project [project_id#17, name#18, desc#19, goal#136, keywords#21, disable_communication#22, country#23, currency#24, deadline#151, cast(state_changed_at#26 as int) AS state_changed_at#166, created_at#27, launched_at#28, backers_count#29, final_status#30]\n               +- Project [project_id#17, name#18, desc#19, goal#136, keywords#21, disable_communication#22, country#23, currency#24, cast(deadline#25 as int) AS deadline#151, state_changed_at#26, created_at#27, launched_at#28, backers_count#29, final_status#30]\n                  +- Project [project_id#17, name#18, desc#19, cast(goal#20 as int) AS goal#136, keywords#21, disable_communication#22, country#23, currency#24, deadline#25, state_changed_at#26, created_at#27, launched_at#28, backers_count#29, final_status#30]\n                     +- Relation [project_id#17,name#18,desc#19,goal#20,keywords#21,disable_communication#22,country#23,currency#24,deadline#25,state_changed_at#26,created_at#27,launched_at#28,backers_count#29,final_status#30] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m dfcasted\u001b[39m.\u001b[39mgroupBy(df\u001b[39m.\u001b[39mcountry)\u001b[39m.\u001b[39mcount()\u001b[39m.\u001b[39morderBy(\u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mshow(\u001b[39m100\u001b[39m)\n\u001b[1;32m      6\u001b[0m dfcasted\u001b[39m.\u001b[39mgroupBy(df\u001b[39m.\u001b[39mcurrency)\u001b[39m.\u001b[39mcount()\u001b[39m.\u001b[39morderBy(\u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mshow(\u001b[39m100\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m dfcasted\u001b[39m.\u001b[39;49mgroupBy(df\u001b[39m.\u001b[39;49mstate_changed_at)\u001b[39m.\u001b[39;49mcount()\u001b[39m.\u001b[39morderBy(\u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mshow(\u001b[39m100\u001b[39m)\n\u001b[1;32m      8\u001b[0m dfcasted\u001b[39m.\u001b[39mgroupBy(\u001b[39m\"\u001b[39m\u001b[39mbackers_count\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcount()\u001b[39m.\u001b[39morderBy(\u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mshow(\u001b[39m100\u001b[39m)\n\u001b[1;32m      9\u001b[0m dfcasted\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mgoal\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfinal_status\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mshow(\u001b[39m30\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pyspark/sql/group.py:39\u001b[0m, in \u001b[0;36mdfapi.<locals>._api\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_api\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mGroupedData\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m     38\u001b[0m     name \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m---> 39\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jgd, name)()\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Resolved attribute(s) state_changed_at#26 missing from project_id#17,name#18,desc#19,goal#136,keywords#21,disable_communication#22,country#23,currency#24,deadline#151,state_changed_at#166,created_at#181,launched_at#196,backers_count#211,final_status#226 in operator !Aggregate [state_changed_at#26], [state_changed_at#26, count(1) AS count#750L]. Attribute(s) with the same name appear in the operation: state_changed_at. Please check if the right attribute(s) are used.;\n!Aggregate [state_changed_at#26], [state_changed_at#26, count(1) AS count#750L]\n+- Project [project_id#17, name#18, desc#19, goal#136, keywords#21, disable_communication#22, country#23, currency#24, deadline#151, state_changed_at#166, created_at#181, launched_at#196, backers_count#211, cast(final_status#30 as int) AS final_status#226]\n   +- Project [project_id#17, name#18, desc#19, goal#136, keywords#21, disable_communication#22, country#23, currency#24, deadline#151, state_changed_at#166, created_at#181, launched_at#196, cast(backers_count#29 as int) AS backers_count#211, final_status#30]\n      +- Project [project_id#17, name#18, desc#19, goal#136, keywords#21, disable_communication#22, country#23, currency#24, deadline#151, state_changed_at#166, created_at#181, cast(launched_at#28 as int) AS launched_at#196, backers_count#29, final_status#30]\n         +- Project [project_id#17, name#18, desc#19, goal#136, keywords#21, disable_communication#22, country#23, currency#24, deadline#151, state_changed_at#166, cast(created_at#27 as int) AS created_at#181, launched_at#28, backers_count#29, final_status#30]\n            +- Project [project_id#17, name#18, desc#19, goal#136, keywords#21, disable_communication#22, country#23, currency#24, deadline#151, cast(state_changed_at#26 as int) AS state_changed_at#166, created_at#27, launched_at#28, backers_count#29, final_status#30]\n               +- Project [project_id#17, name#18, desc#19, goal#136, keywords#21, disable_communication#22, country#23, currency#24, cast(deadline#25 as int) AS deadline#151, state_changed_at#26, created_at#27, launched_at#28, backers_count#29, final_status#30]\n                  +- Project [project_id#17, name#18, desc#19, cast(goal#20 as int) AS goal#136, keywords#21, disable_communication#22, country#23, currency#24, deadline#25, state_changed_at#26, created_at#27, launched_at#28, backers_count#29, final_status#30]\n                     +- Relation [project_id#17,name#18,desc#19,goal#20,keywords#21,disable_communication#22,country#23,currency#24,deadline#25,state_changed_at#26,created_at#27,launched_at#28,backers_count#29,final_status#30] csv\n"
     ]
    }
   ],
   "source": [
    "#dfcasted.groupBy(df.disable_communication).count().orderBy(\"count\", ascending=False).show(100)\n",
    "\n",
    "dfcasted.select(\"deadline\").dropDuplicates().show()\n",
    "\n",
    "dfcasted.groupBy(df.country).count().orderBy(\"count\", ascending=False).show(100)\n",
    "dfcasted.groupBy(df.currency).count().orderBy(\"count\", ascending=False).show(100)\n",
    "dfcasted.groupBy(df.state_changed_at).count().orderBy(\"count\", ascending=False).show(100)\n",
    "dfcasted.groupBy(\"backers_count\").count().orderBy(\"count\", ascending=False).show(100)\n",
    "dfcasted.select(\"goal\", \"final_status\").show(30)\n",
    "dfcasted.groupBy(\"country\", \"currency\").count().orderBy(\"count\",ascending=False).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlevez la colonne disable_communication. Cette colonne est très largement majoritairement à false, il n'y a que 322 true (négligeable), le reste est non-identifié :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dfcasted.drop(\"disable_communication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les fuites du futur\n",
    "\n",
    "Dans les datasets construits a posteriori des évènements, il arrive que des données ne pouvant être connues qu'après la résolution de chaque évènement soient insérées dans le dataset. On a des fuites depuis le futur ! Par exemple, on a ici le nombre de \"backers\" dans la colonne backers_count. Il s'agit du nombre total de personnes ayant investi dans chaque projet, or ce nombre n'est connu qu'après la fin de la campagne.\n",
    "\n",
    "Il faut savoir repérer et traiter ces données pour plusieurs raisons :\n",
    "\n",
    "- Pendant l'entraînement (si on ne les a pas enlevées) elles facilitent le travail du modèle puisqu'elles contiennent des informations directement liées à ce qu'on veut prédire. Par exemple, si backers_count = 0 on est sûr que la campagne a raté.\n",
    "- Au moment d'appliquer notre modèle, les données du futur ne sont pas présentes (puisqu'elles ne sont pas encore connues). On ne peut donc pas les utiliser comme input pour un modèle.\n",
    "\n",
    "Ici, pour enlever les données du futur on retire les colonnes backers_count et state_changed_at :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoFuture = df2.drop(\"backers_count\", \"state_changed_at\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colonnes currency et country\n",
    "\n",
    "On pourrait penser que les colonnes currency et country sont redondantes, auquel cas on pourrait enlever une des colonnes. Mais c'est oublier par exemple que tous les pays de la zone euro ont la même monnaie ! Il faut donc garder les deux colonnes.\n",
    "\n",
    "Il semble y avoir des inversions entre ces deux colonnes et du nettoyage à faire. On remarque en particulier que lorsque country = \"False\" le country à l'air d'être dans currency. On le voit avec la commande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|currency|count|\n",
      "+--------+-----+\n",
      "|      US|  405|\n",
      "|     NOK|  113|\n",
      "|      GB|   13|\n",
      "|      CA|    3|\n",
      "|      AU|    3|\n",
      "|      NL|    2|\n",
      "|      NZ|    1|\n",
      "|      NO|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.country == False) \\\n",
    "  .groupBy(\"currency\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"count\", ascending = False) \\\n",
    "  .show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créez deux udfs nommées udf_country et udf_currency telles que :\n",
    "- cleanCountry : si country = \"False\" prendre la valeur de currency, sinon si country est une chaîne de caractères de taille autre que 2 remplacer par null, et sinon laisser la valeur country actuelle. On veut les résultat dans une nouvelle colonne country2.\n",
    "- cleanCurrency : si currency.length != 3 currency prend la valeur null, sinon laisser la valeur currency actuelle. On veut les résultats dans une nouvelle colonne currency2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanCountry(country: str, currency: str) -> str:\n",
    "  if (country == \"False\"):\n",
    "    currency\n",
    "  else:\n",
    "    country\n",
    "\n",
    "def cleanCurrency(currency: str) -> str:\n",
    "  if (currency != None and len(currency) != 3):\n",
    "    return None\n",
    "  else:\n",
    "    return currency\n",
    "\n",
    "cleanCountryUdf = udf(cleanCountry)\n",
    "cleanCurrencyUdf = udf(cleanCurrency)\n",
    "\n",
    "dfCountry = dfNoFuture.withColumn(\"country2\", cleanCountryUdf(\"country\", \"currency\")) \\\n",
    "                      .withColumn(\"currency2\", cleanCurrencyUdf(\"currency\")) \\\n",
    "                      .drop(\"country\", \"currency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a montré ici l'utilisation d'udfs, mais de façon générale toujours privilégier les fonctions déjà codées dans Spark car elles sont optimisées.\n",
    "\n",
    "Pour une classification, l’équilibrage entre les différentes classes cibles dans les données d’entraînement doit être contrôlé (et éventuellement corrigé). Affichez le nombre d’éléments de chaque classe (colonne final_status). Conservez uniquement les lignes qui nous intéressent pour le modèle, à savoir lorsque final_status vaut 0 (Fail) ou 1 (Success). Les autres valeurs ne sont pas définies et on les enlève. On pourrait toutefois tester en mettant toutes les autres valeurs à 0 en considérant que les campagnes qui ne sont pas un Success sont un Fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajouter et manipuler des colonnes\n",
    "\n",
    "Il est parfois utile d’ajouter des features (colonnes dans un DataFrame) pour aider le modèle lors de son apprentissage. Ici nous allons créer de nouvelles features à partir de celles déjà présentes dans les données. Dans certains cas on peut ajouter des features en allant chercher des sources de données supplémentaires.\n",
    "\n",
    "Les dates ne sont pas directement exploitables par un modèle sous leur forme initiale dans nos données : il s’agit de timestamps Unix (nombre de secondes depuis le 1er janvier 1970 0h00 UTC). Nous allons traiter ces données pour en extraire des informations pour aider les modèles. Nous allons, entre autres, nous servir des fonctions liées aux dates de l'objet functions.\n",
    "\n",
    "Ajoutez une colonne days_campaign qui représente la durée de la campagne en jours (le nombre de jours entre launched_at et deadline).\n",
    "\n",
    "Ajoutez une colonne hours_prepa qui représente le nombre d’heures de préparation de la campagne entre created_at et launched_at. On pourra arrondir le résultat à 3 chiffres après la virgule.\n",
    "\n",
    "Supprimez les colonnes launched_at, created_at, et deadline, elles ne sont pas exploitables pour un modèle.\n",
    "\n",
    "Pour exploiter les données sous forme de texte, nous allons commencer par réunir toutes les colonnes textuelles en une seule. En faisant cela, on rend indiscernable le texte du nom de la campagne, de sa description et des keywords, ce qui peut avoir des conséquences sur la qualité du modèle. Mais on cherche à construire ici un premier benchmark de modèle, avec une solution simple qui pourra servir de référence pour des modèles plus évolués.\n",
    "\n",
    "Mettre les colonnes name, desc, et keywords en minuscules.\n",
    "\n",
    "Ajoutez une colonne text, qui contient la concaténation des Strings des colonnes name, desc, et keywords. ATTENTION à bien mettre des espaces entre les chaînes de caractères concaténées, car on fera par la suite un split en se servant des espaces entre les mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs nulles\n",
    "\n",
    "Il y plusieurs façons de traiter les valeurs nulles pour les rendre exploitables par un modèle. Nous avons déjà vu que parfois les valeurs nulles peuvent être comblées en utilisant les valeurs d’une autre colonne (parce que le dataset a été mal préparé). On peut aussi décider de supprimer les exemples d’entraînement contenant des valeurs nulles, mais on risque de perdre beaucoup de données. On peut également les remplacer par la valeur moyenne ou médiane de la colonne. On peut enfin leur attribuer une valeur particulière, distincte des autres valeurs de la colonne.\n",
    "\n",
    "Remplacez les valeurs nulles des colonnes days_campaign, hours_prepa, et goal par la valeur -1 et par \"unknown\" pour les colonnes country2 et currency2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarder un DataFrame\n",
    "\n",
    "Sauvegarder le DataFrame final au format parquet sur votre machine :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/27 21:10:06 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "22/11/27 21:10:06 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n",
      "22/11/27 21:10:07 WARN MemoryManager: Total allocation exceeds 95,00% (906 992 014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfCountry.write.parquet(\"/home/mgard/Documents/pyspark/cours-spark-telecom/data/out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention ! Lorsqu’on sauvegarde un output en Spark, le résultat est toujours un répertoire contenant un ou plusieurs fichiers. Cela est dû à la nature distribuée de Spark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit ('3.10.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d49593f0c9713db4f64deb16538b3520ece0d230c333a218818733e3164f464"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
